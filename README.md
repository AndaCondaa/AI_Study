# DL 정리

## Loss Function

- 신경망은 결국 Loss Function을 가지고 손실이 최소가 되는 매개변수, 즉 가중치를 찾는 과정

- <b>특정 매개변수일 때,  정해지는 손실함수의 미분에 집중</b>❗️

- 정확도를 지표로 삼으면, 거의 모든 곳에서 미분값이 0이기 때문에 학습에 사용할 수 없음, 즉 손실함수의 미분가능 연속성때문에 사용함 (활성화함수로 계단함수를 생각)

- 종류
  - 오차제곱합(SSE)
    - $E =\frac{1}{2} \sum(y_k-t_k)^2$
    - 분류 문제일 경우?
      - 모든 타겟(타겟종류)에 대한 Sigmoid 또는 Softmax 값을 가지고 적용하기 때문에 분류에서는 잘 사용 ❌
  - Cross Entropy Error (CEE)
    - $E = - \sum t_k\log y_k$
    - 분류에서 원핫인코딩 된 경우, 정답값이 아닌 레이블은 $t_k$가 0이 되므로, 정답 레이블로 추정한 확률로 확인하는 개념
  - Binary Cross Entropy (BCE)
    - 입력 데이터가 베르누이 형태일 때(즉, 0과 1에 거의 붙어 있을 때 = 정규분포가 아닐 때) BCE를 비용함수로 사용
    - 활성화 함수로 시그모이드를 사용했을 때, 비용함수의 입력값(즉, 시그모이드 출력값)은 0~1사이임. 이 경우 오차제곱법을 사용하면 비효율적임. 예측값과 실젯값을 연산하기 때문에 예측값도 0~1, 실제값도 0~1이라서 차이가 작아서!

<br>

<br>

## Activations Function

- Sigmoid Function 
  - 출력값이 0~1 범위리기 때문에 매우 큰 입력ㄱ밧이 입력돼도 최대 1의 값을 갖게 되어 기울기 소실문제 발생 (대신, 기울기 폭주 문제는 없음)
  - 출력값의 중심이 0이 아니므로 입력 데이터가 항상 양수인 경우라면, 기울기는 모두 양수 또는 음수가 되어 기울기가 지그재그 형태로 변동하는 문제점이 발생 -> 학습 효율성 감소
  - 계층이 많아지면서 기울기가 0에 수렴하는 문제가 발생해서, 은닉층에서는 사용하지 않고, 주로 출력층에서만 사용
- Hyperbolic Tangent Function
  - 시그모이드와 유사하지만 출력값의 중심이 0, 기울기 소실이 비교적 덜 발생함
  - 하지만 입력값이 +-4보다 넓은 범위에서 기울기가 0으로 수렴하기 때문에 기울기 소실문제는 여전히 발생
- ReLU (Rectified Linear Unit Function)
  - 입력값이 양수라면 기울기 소실이 발생하지 않음
  - 입력값이 음수인 경우 항상 0을 반환하므로 가중치나 평향이 갱신되지 않을 수 있음. 가중치의 합이 음수가 되면, 해당 노드는 더 이상 갱신하지 않아 Dead Neuron이 됨.
- LeakyReLU (Leaky Rectified Linear Unit Function)
  - 음수 기울기를 제어하여 Dead Neuron 현상을 방지함
- PReLU (Parametric Rectified Linear Unit Function)
  - LeakyReLU와 형태가 동일하지만, negative slope(입력이 음수일 때의 기울기)를 고정값이 아닌, 학습을 통해 갱신되는 값으로 간주함
- ELU (Exponential Linear Unit Function)
  - 입력값이 0인 지점에서도 끊어지지 않아서 출력값이 급변하지 않음. 그래서 경사 하강법의 수렴 속도가 비교적 빠름, but 학습속도는 느림
- Softmax Function
  - 클래스에 속할 확률을 계산하는 것이므로, 은닉층에서 사용하지 않고, 출력측에서 사용함

<br>

<br>

##  Normalization

Pytorch 정규화 클래스를 이용

- 배치 정규화
  - 미니 배치에서 계산된 평균 및 분산을 기반으로 계층의 입력을 정규화
  - 순방향 신경망(Feedforward Nerual Net)에서 주로 사용 (CNN, MLP 등)
  - $\mu_B ← \frac{1}{m} \sum{x_i}$
  - $\sigma_B^2 ← \frac{1}{m} \sum(x_i - \mu_B)^2$
  - $\hat{x_i} ← \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
- 계층 정규화
  - 채널 축으로 정규화됨
  - 배치 샘플 간의 의존관계가 없음
  - RNN, 트랜스포머 기반 모델에서 주로 사용
- 인스턴스 정규화
  - 채널과 샘플을 기준으로 정규화
  - 각 샘플에 대해 개별적으로 수행되므로 입력이 다른 분포를 갖는 작업에 적합
  - GAN, Style Transfer 모델에 주로 사용
- 그룹 정규화
  - 채널을 N개의 그룹으로 나누어서 정규화
  - CNN의 배치 크기가 작을 때 대안으로 사용

<br>

<br>

## Weight Initialization

- Xavier (제이비어) 초기화
  - 각 노드의 출력 분산이 입력 분산과 동일하도록 초기화
  - 은닉층의 노드 수에 따라 다른 표준 편차를 할당한다는 특징
  - 입력 데이터의 분산이 출력 데이터에서 유지됨
  - <b>시그모이드나 하이퍼볼릭 탄젠트를 사용할 때 효과적</b>❗️
- 카이밍/허 초기화
  - 각 노드의 출력 분산이 입력 분산과 동일하도록 초기화
  - 순방향 신경망에서 효과적
  - <b>ReLU 함수를 사용할 때 효과적</b>❗️
- 직교 초기화 (Orthogonal)
  - 특이값 분해 (SVD, Singular Value Decomposition)를 활용하여 자기 자신을 제외한 나머지 모든 열, 행 벡터들과 직교이면서 동시에 단위 벡터인 행렬을 만드는 방법
  - 직교 행렬의 고윳값의 절대값은 1이기 때문에, 행렬 곱을 여러 번 수행하더라도 기울기 폭주나 소실이 발생하지 않음
  - <b>LSTM, GRU(Gated Recurrent Units)과 같은 RNN에서 주로 사용됨</b>
  - <b>모델이 특정 초기화 값에 지나치게 민감해지므로 순방향 신경망에서는 사용❌</b>

<br>

<br>

## Regularization

- L1 (Lasso)
  - 가중치 절댓값의 합
  - 가중치가 0이 될 수 있음 -> 특성 선택
  - 이상치에 강함
- L2 (Ridge)
  - 가중치 제곱의 합
  - 가중치가 0이 되지 않음
  - 복잡한 신경망에서 사용
- Weight Decay
  - 파이토치에서는 L2 규제를 weight_decay 매개변수로 사용
  - 0~1 사이의 하이퍼파라미터 설정 가능 (0일 경우, 감쇠효과X)
- Momentum
  - 기존 SGD에서 관성을 추가한 것 - 이전 가중치에서의 그레디언트를 반영
  - (이전 가중치에서의 그레디언트 * 모멘텀 계수)를 더해서 사용
  - 모멘텀 계수는 momentum 하이퍼파라미터 사용 (0일 경우, SGD와 동일)
- Elastic-Net
  - L1 + L2
  - L1, L2의 비율 나누어 모두 사용
- Dropout
  - 일부 노드를 제거하거나 0으로 설정하여, 노드 간 의존성을 억제함
  - 데이터셋이 많지 않으면 성능이 저하될 수 있음
  - <b>드롭아웃과 배치 정규화를 함께 사용❌❌</b>
  - 함께 사용하는 경우는 드롭아웃 -> 배치 정규화 순으로 적용해야 함.
  - 영상이나 음성에서 좋음
- Gradient Clipping
  - L2 노름을 사용하여 기울기의 방향을 그대로 하면서, 최대 임계값을 넘지 않도록 조절 - 기울기 폭주문제 억제
  - 최대 임계값 r은 0.1이나 1과 같이 작은 크기를 적용
  - 역전파 함수와 최적화 함수 사이에서 호출 (역전파를 이용해서 기울기를 먼저 구해야하고, 이후에 클리핑 된 것을 가지고 Optimizer를 적용해야 하기 때문)
  - RNN, LSTM 모델을 학습하는 데 주로 사용

<br>

<br>

## Data Augmentation (Text - NLPAUG)

- NLPAUG 라이브러리에서 다양한 클래스 제공
  - 삽입/삭제
  - 교체/대체
- 역번역
  - 특정 언어로 번역한 다음 다시 본래의 언어로 번역하는 방법
  - Paraphrasing(패러프레이징) 효과를 얻을 수 있음
    - 앞에서 사용한 단어 중 뜻이 같거나 유사한 어휘를 사용해 문장을 바꿔 표현하는 것
  - BackTranslationAug 클래스

<br>

<br>

## Data Augmentation (Image - torchvision/imgaug)

- 변환 적용 방법
  - Torchvision의 transforms 모듈 사용
  - Compose 클래스 사용-> Seuential과 같은 역할
    - 어떠한 순서로 진행하는가에 따라 데이터가 달라질 수 있기 때문에 Compose로 묶어서 사용 및 일괄 적용
  - PIL 이미지를 transforms.ToTensor 함수로 텐서 형식으로 변환
    - 0.0~1.0으로 최대 최소 정규화됨
    - [C, H, W] 형태로 변환함
- 회전 및 대칭
- 자르기 및 패딩
- 크기 조정
- 변형
- 색상 변환
- 노이즈
  - 테스트 데이터에도 노이즈를 추가해서 일반화 능력이나 강건성을 평하는 데 사용 가능
  - imgaug 라이브러리 사용
    - ndarray를 입력으로 사용함, 호출 함수에 변환/복구 과정을 추가함
    - 클래스를 만들어서 사용 -> 클래스를 정의하고 torchvision Compose에서 호출
    - 초기화 메서드에서 Sequential로 노이즈 처리들을 정의하고, 호출 메서드에서 처리 진행
- Cutout/Random Erasing
  - Cutout
    - 임의의 사각형 영역을 삭제하고 0의 픽셀값으로 채움
    - 동영상에서 Occlusion(폐색 영역, 특정 프레임에는 존재하지만 다른 프레임에는 존재하지 않는 영역)에 대해 모델이 더 강건하게 해줌 
  - Random Erasing
    - 임의의 사각형 영역을 삭제하고 무작위 픽셀값으로 채움
    - 일부 영역이 누락되거나 잘렸을 때 더 강건한 모델을 만들 수 있게 해줌
- Mixup/CutMix
  - Mixup
    - 단순히 두 이미지를 선형으로 결합
    - 레이블링이 다르게 태깅돼 있어도 더 낮은 오류를 보임, 다중 레이블 문제에 대해서도 더 견고함
    - 단순히 numpy로 클래스로 만들어서 구현하고 torchvision Compose에서 호출
  - CutMix
    - Patch 영영에 다른 이미지를 덮어 씌우는 방식
    - 모델이 이미지의 특정 영역을 기억해 인식하는 문제를 완화
    - 이미지 전체를 보고 판단할 수 있게 일반화

<br>

<br>



<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

## CNN

- 기본 개념
  - Affine에서는 이미지를 1차원으로 평탄화하여 사용하기 때문에, 이미지의 형상은 무시됨.
  - 패딩 : 합성곱 신경망에서 출력 크기를 조정할 목적
  - 스트라이드(보폭) : 필터를 적용하는 위치의 간격
  - 입출력 크기
    - 입력 $(H, W)$, 커널 $(FH, FW)$, 출력 $(OH, OW)$, 패딩 $P$, 스트라이드 $S$
    - $OH = \frac{H+2P-FH}{S} + 1$
    - $OW = \frac{W+2P-FW}{S} + 1$
  - 3차원 데이터에서는 입력 데이터의 채널 수와 커널의 채널 수를 같게 하면 됨. 그리고 똑같이 FMA하여 더해줌 -> 이 경우에는 출력 특징맵이 1채널이 됨
- 3차원 데이터 처리
  - 필터의 개수를 늘리면 출력맵도 같은 개수로 출력됨. 
  - 3채널의 입력을 블록으로 생각하고, 하나의 커널을 3채널짜리 커널로 생각하면 이를 계산하면 1채널짜리 출력맵이 출력됨. 따라서 3채널짜리 커널을 여러개 적용하면 출력맵의 채널도 늘어남 <b><필터의 수도 고려해야함!></b>
  - 커널의 가중치 데이터 (출력 채널 수(커널개수), 입력 채널 수 , 높이, 너비)
  - 이 경우 편향은 (출력 채널 수, 1, 1)이 됨.
- 풀링 계층
  - 가로/세로 공간을 줄이는 연산
  - 최대 풀링 / 평균 풀링
    - 주로 최대 풀링을 사용

- 기본 흐름
  - 합성곱 계층을 통해서 배치 데이터의 공간을 줄여서 특징 맵을 만들고, 이후에 Affine계층을 추가해서 학습
  - 커널의 값과 Affine계층의 가중치가 손실함수를 기준으로 학습됨 (기본적인 신경망과 동일)

<br>

<br>

## Pre-trained Model

- 사전 학습된 모델 자체를 적용하거나 / 사전 학습된 임베딩 벡터를 활용

  - 임베딩 벡터 : 입력 데이터를 연속적이고 조밀한 벡터로 매핑하는 것

- 전이 학습에 사용하거나, 백본 네트워크로 사용됨

- #### <b>Backbone Network</b>

  - 입력 데이터에서 특징을 추출해서 최종 분류기에 전달하는 모델 혹은 그 일부
  - 최종 추정기로 가기 전에 입력 이미지의 특징 추출기의 역할로 사용할 수 있음
  - 백본으로 BERT, GPT, VGG-16, ResNet과 같은 모델을 가져와서,  Transfer Learning을 통해 최종 모델을 만들 수 있음

- #### <b>Transfer Learning (전이 학습)</b>

  - 



























































