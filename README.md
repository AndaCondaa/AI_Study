# DL 정리

## Loss Function

- 신경망은 결국 Loss Function을 가지고 손실이 최소가 되는 매개변수, 즉 가중치를 찾는 과정

- <b>특정 매개변수일 때,  정해지는 손실함수의 미분에 집중</b>❗️

- 정확도를 지표로 삼으면, 거의 모든 곳에서 미분값이 0이기 때문에 학습에 사용할 수 없음, 즉 손실함수의 미분가능 연속성때문에 사용함 (활성화함수로 계단함수를 생각)

- 종류
  - 오차제곱합(SSE)
    - $E =\frac{1}{2} \sum(y_k-t_k)^2$
    - 분류 문제일 경우?
      - 모든 타겟(타겟종류)에 대한 Sigmoid 또는 Softmax 값을 가지고 적용하기 때문에 분류에서는 잘 사용 ❌
  - Cross Entropy Error (CEE)
    - $E = - \sum t_k\log y_k$
    - 분류에서 원핫인코딩 된 경우, 정답값이 아닌 레이블은 $t_k$가 0이 되므로, 정답 레이블로 추정한 확률로 확인하는 개념
  - Binary Cross Entropy (BCE)
    - 입력 데이터가 베르누이 형태일 때(즉, 0과 1에 거의 붙어 있을 때 = 정규분포가 아닐 때) BCE를 비용함수로 사용
    - 활성화 함수로 시그모이드를 사용했을 때, 비용함수의 입력값(즉, 시그모이드 출력값)은 0~1사이임. 이 경우 오차제곱법을 사용하면 비효율적임. 예측값과 실젯값을 연산하기 때문에 예측값도 0~1, 실제값도 0~1이라서 차이가 작아서!

<br>

<br>

## Activations Function

- Sigmoid Function 
  - 출력값이 0~1 범위리기 때문에 매우 큰 입력ㄱ밧이 입력돼도 최대 1의 값을 갖게 되어 기울기 소실문제 발생 (대신, 기울기 폭주 문제는 없음)
  - 출력값의 중심이 0이 아니므로 입력 데이터가 항상 양수인 경우라면, 기울기는 모두 양수 또는 음수가 되어 기울기가 지그재그 형태로 변동하는 문제점이 발생 -> 학습 효율성 감소
  - 계층이 많아지면서 기울기가 0에 수렴하는 문제가 발생해서, 은닉층에서는 사용하지 않고, 주로 출력층에서만 사용
- Hyperbolic Tangent Function
  - 시그모이드와 유사하지만 출력값의 중심이 0, 기울기 소실이 비교적 덜 발생함
  - 하지만 입력값이 +-4보다 넓은 범위에서 기울기가 0으로 수렴하기 때문에 기울기 소실문제는 여전히 발생
- ReLU (Rectified Linear Unit Function)
  - 입력값이 양수라면 기울기 소실이 발생하지 않음
  - 입력값이 음수인 경우 항상 0을 반환하므로 가중치나 평향이 갱신되지 않을 수 있음. 가중치의 합이 음수가 되면, 해당 노드는 더 이상 갱신하지 않아 Dead Neuron이 됨.
- LeakyReLU (Leaky Rectified Linear Unit Function)
  - 음수 기울기를 제어하여 Dead Neuron 현상을 방지함
- PReLU (Parametric Rectified Linear Unit Function)
  - LeakyReLU와 형태가 동일하지만, negative slope(입력이 음수일 때의 기울기)를 고정값이 아닌, 학습을 통해 갱신되는 값으로 간주함
- ELU (Exponential Linear Unit Function)
  - 입력값이 0인 지점에서도 끊어지지 않아서 출력값이 급변하지 않음. 그래서 경사 하강법의 수렴 속도가 비교적 빠름, but 학습속도는 느림
- Softmax Function
  - 클래스에 속할 확률을 계산하는 것이므로, 은닉층에서 사용하지 않고, 출력측에서 사용함

<br>

<br>

##  Normalization

Pytorch 정규화 클래스를 이용

- 배치 정규화
  - 미니 배치에서 계산된 평균 및 분산을 기반으로 계층의 입력을 정규화
  - 순방향 신경망(Feedforward Nerual Net)에서 주로 사용 (CNN, MLP 등)
- 계층 정규화
  - 채널 축으로 정규화됨
  - 배치 샘플 간의 의존관계가 없음
  - RNN, 트랜스포머 기반 모델에서 주로 사용
- 인스턴스 정규화
  - 채널과 샘플을 기준으로 정규화
  - 각 샘플에 대해 개별적으로 수행되므로 입력이 다른 분포를 갖는 작업에 적합
  - GAN, Style Transfer 모델에 주로 사용
- 그룹 정규화
  - 채널을 N개의 그룹으로 나누어서 정규화
  - CNN의 배치 크기가 작을 때 대안으로 사용

<br>

<br>

## Weight Initialization

- Xavier (제이비어) 초기화
  - 각 노드의 출력 분산이 입력 분산과 동일하도록 초기화
  - 은닉층의 노드 수에 따라 다른 표준 편차를 할당한다는 특징
  - 입력 데이터의 분산이 출력 데이터에서 유지됨
  - <b>시그모이드나 하이퍼볼릭 탄젠트를 사용할 때 효과적</b>❗️
- 카이밍/허 초기화
  - 각 노드의 출력 분산이 입력 분산과 동일하도록 초기화
  - 순방향 신경망에서 효과적
  - <b>ReLU 함수를 사용할 때 효과적</b>❗️
- 직교 초기화 (Orthogonal)
  - 특이값 분해 (SVD, Singular Value Decomposition)를 활용하여 자기 자신을 제외한 나머지 모든 열, 행 벡터들과 직교이면서 동시에 단위 벡터인 행렬을 만드는 방법
  - 직교 행렬의 고윳값의 절대값은 1이기 때문에, 행렬 곱을 여러 번 수행하더라도 기울기 폭주나 소실이 발생하지 않음
  - <b>LSTM, GRU(Gated Recurrent Units)과 같은 RNN에서 주로 사용됨</b>
  - <b>모델이 특정 초기화 값에 지나치게 민감해지므로 순방향 신경망에서는 사용❌</b>

<br>

<br>

## Regularization





<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

## Backpropagation

수치미분 대신 손실함수의 기울기를 계산하는 방법 → 해석적 방법

<br>

<br>

## Optimization

- SGD
  - $W ← W - \eta \frac{\partial L}{\partial W}$
  - 문제에 따라 지그재그 형태를 보이면서 비효율적으로 찾아가는 문제가 있음
- Momentum
  - $v ← \alpha v - \eta \frac{\partial L}{\partial W}$ 
  - $W ← W + v$
- AdaGrad
  - $h ← h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$ 
  - $W ← W - \eta \frac{1}{\sqrt h}\frac{\partial L}{\partial W}$ 
  - Learning rate decay 기술을 적용
  - 개별 매개변수에 <b>Ada</b>ptive하게 학습률을 조정하면서 진행
  - h의 역수를 곱해주고 있기 때문에, 크게 갱신된 매개변수는 학습률이 점차 작아짐
- Adam
  - Mometum과 AdaGrad를 융합한 듯한 방법
  - ❗️❗️논문 참고

<br>

<br>

## 학습 관련 기술

- <b>가중치 초기값 설정</b>
  - 일반적으로 가중치를 균일한 값으로 설정하면, 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되는 문제 발생 -> 무작위로 설정해야 함
  - 문제
    - Gradient vanishing
      - 가중치를 표준편차 1인 정규분포를 사용할 때,  활성화 함수의 출력이 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라짐
    - 표현력 제한
      - 표준편차를 줄이면 gradient vanishing 문제는 사라지지만, 뉴런에서 모두 비슷한 값을 출력하면서 표현력을 제한함
  - Xavier 초기값 (제이비어 초기값)
    - <b>각 노드의 출력 분산이 입력 분산과 동일하도록 초기화</b>
    - 앞 층의 입력 노드 수와 다음 층의 출력 노드 수를 고려한 설정값 제안
    - 간단하게는 앞 계층의 노드가 $n$개일 때, 표준편차가 $\frac{1}{\sqrt n}$인 정규분포를 사용
    - 활성화 함수가 선형이라는 전제로 이끈 결과이기때문에,  tanh, sigmoid 등 일 때는 중앙 부근이 선형이라고 볼 수 있어서 적당함, ReLU에서는 부적절❌❌
  - He 초기값 (카이밍 히)
    - 앞 계층의 노드가 $n$개일 때, 표준편차가 $\sqrt \frac{2}{n}$인 정규분포를 사용
    - 순방향 신경망 네트워크에서 효과적
    - <b>각 노드의 출력 분산이 입력 분산과 동일하게 만들어 ReLU 함수의 죽은 뉴런 문제를 최소화할 수 있음. ReLU 사용 시 효과적</b>

<br>

- <b>Batch Normalization</b>
  - $\mu_B ← \frac{1}{m} \sum{x_i}$
  - $\sigma_B^2 ← \frac{1}{m} \sum(x_i - \mu_B)^2$
  - $\hat{x_i} ← \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
  - 각 층에서의 활성화값이 적당히 분포되도록 조정하는 것 (각 층이 활성화를 적당히 퍼뜨리도록 '강제'하는 것)
  - 특징
    - 학습을 빨리 진행할 수 있음
    - 초기값에 크게 의존하지 않음 (초기값 선택에 대한 문제 해결)
    - 과대적합을 억제함 (드롭아웃 등의 필요성 감소)
  - Batch Norm(배치 정규화 계층)을 신경망에 삽입함
    - $y_i ← \gamma \hat{x_i} + \beta$
    - Affine -> Batch Norm -> Activation Function
    - 미니 배치를 단위로 정규화를 함
    - 또, 배치 정규화 계층마다 이 정규화된 데이터에 고유한 scale(확대, $\gamma$)와 shift(이동, $\beta$) 변환을 수행. (학습하면서 적합한 값으로 조정해감)

<br>

- <b>Dropout</b>
  - Weight decay 방법으로 L2 Norm을 사용할 수 있지만, 신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어렵기 때문에 드롭아웃을 사용
  - 뉴런을 임의로 삭제하면서 학습하는 방법
  - 훈련때는 임의로 데이터를 흘릴 뉴런을 삭제, 시험 때는 모든 뉴런에 신호를 전달

<br>

- <b>하이퍼파라미터 찾기</b>
  - 데이터 분리
    - 훈련 데이터 : 매개변수 학습
    - 검증 데이터 : 하이퍼파리미터 성능 평가
    - 시험 데이터 : 신경망의 범용 성능 평가
  - 신경망에서는 그리드 서치 같은 규칙적인 탐색보다는 무작위로 샘플링해서 범위를 좁혀가는 것이 좋음. 최종 정확도에 미치는 영향력이 하이퍼파라미터마다 다르기 때문

<br>

<br>

## CNN

- 기본 개념
  - Affine에서는 이미지를 1차원으로 평탄화하여 사용하기 때문에, 이미지의 형상은 무시됨.
  - 패딩 : 합성곱 신경망에서 출력 크기를 조정할 목적
  - 스트라이드(보폭) : 필터를 적용하는 위치의 간격
  - 입출력 크기
    - 입력 $(H, W)$, 커널 $(FH, FW)$, 출력 $(OH, OW)$, 패딩 $P$, 스트라이드 $S$
    - $OH = \frac{H+2P-FH}{S} + 1$
    - $OW = \frac{W+2P-FW}{S} + 1$
  - 3차원 데이터에서는 입력 데이터의 채널 수와 커널의 채널 수를 같게 하면 됨. 그리고 똑같이 FMA하여 더해줌 -> 이 경우에는 출력 특징맵이 1채널이 됨
- 3차원 데이터 처리
  - 필터의 개수를 늘리면 출력맵도 같은 개수로 출력됨. 
  - 3채널의 입력을 블록으로 생각하고, 하나의 커널을 3채널짜리 커널로 생각하면 이를 계산하면 1채널짜리 출력맵이 출력됨. 따라서 3채널짜리 커널을 여러개 적용하면 출력맵의 채널도 늘어남 <b><필터의 수도 고려해야함!></b>
  - 커널의 가중치 데이터 (출력 채널 수(커널개수), 입력 채널 수 , 높이, 너비)
  - 이 경우 편향은 (출력 채널 수, 1, 1)이 됨.
- 풀링 계층
  - 가로/세로 공간을 줄이는 연산
  - 최대 풀링 / 평균 풀링
    - 주로 최대 풀링을 사용

- 기본 흐름
  - 합성곱 계층을 통해서 배치 데이터의 공간을 줄여서 특징 맵을 만들고, 이후에 Affine계층을 추가해서 학습
  - 커널의 값과 Affine계층의 가중치가 손실함수를 기준으로 학습됨 (기본적인 신경망과 동일)

<br>

<br>





























































