{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d368e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e46a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"batch_size\" : 4,\n",
    "    \"learning_rate\" : 0.0001, \n",
    "    \"epochs\" : 5,\n",
    "    \"transform\" : transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48235, 0.45882, 0.40784],\n",
    "                std=[1.0/255.0, 1.0/255.0, 1.0/255.0]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "train_dataset = ImageFolder(\"./datasets/pet/train\", transform=hyperparams[\"transform\"])\n",
    "test_dataset = ImageFolder(\"./datasets/pet/test\", transform=hyperparams[\"transform\"])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True, drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "model = models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\")\n",
    "model.classifier[6] = nn.Linear(4096, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1, Cost : 0.819\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m cost = \u001b[32m0.0\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, classes \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     images = \u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     classes = classes.to(device)\n\u001b[32m     13\u001b[39m     output = model(images)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "for epoch in range(hyperparams[\"epochs\"]):\n",
    "    cost = 0.0\n",
    "    \n",
    "    for images, classes in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        classes = classes.to(device)\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, classes)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cost += loss\n",
    "        \n",
    "    cost = cost / len(train_dataloader)\n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    for images, classes in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        classes = classes.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        outputs_classes = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        accuracy += int(torch.eq(classes, outputs_classes).sum())\n",
    "        print(f\"acc@1 : {accuracy / (len(test_dataloader) * hyperparams['batch_size']) * 100:.2f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./models/VGG16.py\")\n",
    "print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4aa46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
