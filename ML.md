# ML 정리

- Deep Learning 으로 나아가기 위한 ML 기본 지식 정리
- 따라서, DL과 직접적인 연관성이 적은 Tree Model 등은 제외

<br>
<br>

## 필요 ❗️❗️❗️❗️❗️❗️❗️❗️
분류 회귀 등에서 사용되는 로스함수를 정리하자 !<br>
분류 회귀 등에서 사용되는 정확도에 대한 지표를 정리하자 !<br>
그리고 그에따른 tol값 쓰는 것도 이해하자<br>

사이킷런 책 -> 3.5 군집 학습필요❗️❗️❗️




## 메모

- Feature 관련
  - 트리가 아닌 선형/비선형 모델에서는 정규화 필수
  - 0/1 이진 특성의 경우는 그대로 사용
  - 순서형/범주형 변수는 원핫인코딩해서 바이너리로 만드는게 좋음 -> 순서형에서 1,2,3 과 같은 값이 그 사이의 크기를 반영하여 모델이 학습할 수 있음
- 나이브 베이즈 분류
  - 각 특성을 개별로 취급하여 파라미터를 학습하고, 각 특성에서의 클래스 통계를 단순히 취합하는 방식

<br>

<br>

## Regularization

- 과대적합을 억제
- 가중치를 0에 가깝게 -> 특성이 출력에 주는 영향 ⬇️-> 모델의 복잡도 ⬇️  -> 일반화 ⬆️
- Ridge (L2 Regularization)
  - L2 Norm의 제곱을 패널티로 적용
  - 결과 : 부드러운 모델
- Lasso (L1 Regularization)
  - 실제로 0이 되는 계수가 생김 -> Feature selection으로 볼 수 있음
  - 결과 : 희소 모델

<br>
<br>

## SVM

- 매개변수 설정과 데이터 스케일에 아주 민감함

- Kernel trick : 데이터를 고차원 공간에 매필함
  - 다항식 커널 : 특성의 가능한 조합을 지정된 차수까지 모두 계산
  - RBF(radial basis function)(가우시안 커널) : 차원이 무한한 특성 공간에 매핑
- Support Vector = 결정 경계에 위치한 데이터 포인트
- Parameters
  - gamma (가우시안 커널에서 사용)
    - 가우시안 커널 폭의 역수
    - 작은 값은 넓은 영역, 큰 값은 좁은 영역
    - gamma ⬆️ -> 모델 복잡도 ⬆️
  - C
    - 규제
    - 각 포인트의 중요도 (dual_coef_값)을 제한
    - C ⬆️ -> 모델 복잡도 ⬆️
- 비슷한 의미의 특성으로 이루어진 중간 규모 데이터셋에 잘 맞음

- 샘플이 많거나 스케일이 다르면 별로 좋지 않음, 매개변수에 아주 많이 민감함
- BUT, 스케일이 비슷한 (예. 픽셀값) 경우는 괜찮음.

<br>

<br>

## MLP

- 여거 개의 가중치 합을 은닉층을 통해 계산하는 것은 결국 수학적으로는 하나의 가중치 합을 추정하는 것과 같아서, 선형 모델과 같음 -> 따라서 또 다른 기교가 필요 -> 활성화 함수 (activation function) -> 더 복잡한 함수 학습 가능
  - ReLU(rectified linear unit) - (0, 무한)
  - 하이퍼볼릭 탄젠트(hyperbolic tangent) - (-1, 1)
  - Logistic (sigmoid function) - (0, 1) (비추천❌)
- 보통 분류의 경우 신경망 마지막 출력층에 sigmoid/softmax 함수를 적용하여 최종 출력 y를 계산
-  Hidden Unit과 Hidden Layer의 개수가 중요함
- 신경망의 복잡도 제어
  - 은닉층의 수
  - 은닉층의 유닛 개수
  - 규제 (alpha)
  - Dropout - 은닉층 유닛의 일부를 작동시키지 않아서 앙상블시키는 것 같은 효과
  - Early Stop
- 먼저 충분히 과대적합되어서 문제를 해결할 만한 큰 모델 생성 -> 이후, 구조를 줄이거나 규제를 강화해서 일반화 성능 향상

<br>

<br>

## Scaler

<b>데이터셋마다 어떤 것을 사용해야 할지는 다르기 때문에 히스토그램을 확인할 것</b>

- StandardScaler
  - 각 특성의 평균을 0, 분산을 1로 변경
  - 최대/최소값 제한X
- RobustScaler
  - StandardScaler와 비슷하지만, 중간 값과 사분위 값을 사용
  - <b>outlier의 영향을 받지 않음</b>❗️
- MinMaxScaler
  - 0과 1사이에 위치하도록 스케일링
- Normalizer
  - 유클리디안 길이가 1이 되도록 데이터 포인트를 조정
  - 특성 벡터의 길이는 상관없고 데이터의 방향(각도)만이 중요할 때 많이 사용
- QuantileTransformer -> 순위 기반 분위수 변환
  - n_quantiles - 몇 분위로 쪼갤 것인지 
  - output_distribution
    - 'uniform' : 균등 분포, 0에서 1사이에 고르게 분포, 트리 계열에서 주로 사용
    - 'normal' : 정규 분포, MLP 등에서 주로 사용, 분포 왜곡이 있을 때 가장 좋음

- PowerTransformer -> 수학적 함수 기반
  - 'yeo-johnson'
    - 음수 포함 가능
    - 입력에 음수나 0이 있는 경우, 일반적인 머신러닝 전처리에서 사용, MLP와 잘 맞음 (이상치 처리는 못함)
  - 'box-cox'
    - 음수 불가, 데이터 모두 양수일 때만 사용 -> 0보다 엄격히 큰 값만 허용
    - 입력이 모두 양수이고 정규성이 매우 중요할 때 사용

<br>

<br>

## PCA (Principle Component Analysis)

- 데이터의 Principle Component를 구해서 차원을 축소할 수 있음
- 분산의 주요 방향으로 주성분을 구해나가면서 차원을 축소함
- 일반적으로는 특성의 수만큼 주성분이 있어서, 그럴 경우 차원은 그대로지만, 그렇게 쓰지는 않음
- components_ : 각 주성분에 대한 특성 계수값 이 저장됨. 크기는 (주성분 수) X (특성 수)
  - 첫 번째 주성분에 대한 특성 1 계수, 특성 2 계수,...... 
- 데이터에는 기존 특성이 아닌 각 주성분에 대한 계수만 저장
  - 주성분1에 대한 계수, 주성분2에 대한 계수,.....
- explained_variance_ratio (설명된 분산)
  - 각 주성분이 전체 분산을 어느정도 설명하는 지 확인 가능
  - 기본적으로 가장 큰 분산 방향부터 시작하기 때문에, 점점 작아짐
  - 특정 주성분 이상부터는 누적 분산이 거의 1에 가까워 지면서 잘 커지지 않음, 이 정도의 주성분을 정하면 차원은 줄이면서 원본 데이터를 비교적 잘 설명할 수 있음
- n_components 매개변수 
  - 기본적으로는 자연수로 지정하여 몇 개의 주성분을 찾을 지 결정 (기존 Feature의 개수보다 클 수는 없음)
  - 0~1 사이의 값으로 하면, 누적 설명 분산값을 지정하여 주성분 개수를 자동으로 조정 (0.8=80%의 분산을 설명할 수 있을 때까지 주성분을 찾음)

<br>

<br>

## NMF (Non-negative Matrix Factoriztion)

- 비음수 행렬 분해
- <b>주성분과 계수가 모두 0보다 크거나 같음 -> 데이터가 모두 음수가 아닐 때만 적용 가능❗️</b>

- 복원 품질은 PCA보다 조금 떨어짐
- 유용한 패턴을 찾는 데 활용됨
- 소리, 유전자 표현, 텍스트 데이터처럼 덧붙이는 구조를 가진 데이터에 적합

<br>

<br>

## Data Encoding

- <b>ColumnTransformer❗️</b>
  - 데이터프레임 상태 그대로 적용 가능하므로, 컬럼명 확인하면서 전처리 가능 (타겟 컬럼은 분리 후 사용!)
  - 컬럼별로 다르게 데이터 변환을 할 수 있음
  - 파이프라인에 필수
  - remainder=
    - 'drop' : 지정 안 된 컬럼 버림
    - 'passthrough' : 지정 안 된 컬럼은 그대로 통과
  - make_column_transformer는 이름 지정 없이 테스트 용으로 사용가능
- OneHotEncoder, Scaler, Binning을 넣어서 각 컬럼에 대응시켜 사용

<br>

<br>

## Pipeline

- 파이프라인을 사용하는 주된 목적은 그리드 서치 때문❗️
- Pipeline을 통해 하나의 처리기처럼 전처리+모델을 묶을 수 있음
- 그리드서치같은 경우, Scaler를 먼저 하고 그리드서치를 하는 경우에 각 폴드에서 선택되는 검증세트가 결국 초기에 같은 Scaler로 전처리한 상황이 됨. 이 경우 최종 테스트 세트와 다르기 때문에 그리드서치, 즉 교차 검증에서 선택되는 검증세트도 따로 스케일링된 데이터를 사용하는 것이 중요 -> 파이프라인(전처리+모델)을 묶고 파이프라인 자체를 그리드서치에 넣으면, 매 그리드 서치마다 나누어진 검증 폴드에서 선택된 훈련 폴드를 가지고 전처리하고 이를 가지고 검증 폴드를 테스트할 수 있음
- 파이프라인에 들어갈 추정기는 마지막 단계를 제외하고 모두 transform 메서드를 가지고 있어야 함.
- named_steps['이름'] 으로 파이프라인의 추정기 객체에 접근가능
- <b>추가로 전처리기에 있는 매개변수를 선택할 때도 파이프라인을 사용해서 그리드서치가능</b>❗️❗️❗️❗️
  - 예를 들어 PolynomialFeature클래스로 특성을 전처리하고 싶은데, 이 경우 어떤 매개변수로(degree 1 or 2 or 3...) 하는 것이 가장 효과가 좋을지 확인하려면 결국 최종 모델에 넣어보고 평가를 해야함.
  - 따라서 파이프라인에 PolynomialFeature클래스를 추가하고, 그리드서치에 파라미터 셋을 넣어주면 확인할 수 있음
